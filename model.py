# model.py
# XGBoost ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡/ì €ì¥/ë¡œë“œ
# ë ˆë²„ë¦¬ì§€ ETF 5ë¶„ë´‰ ë‹¨íƒ€ì— ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
import os
import pandas as pd
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report


def train_model(df, features):
    """
    XGBoost ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
    - ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€ (shuffle=False)
    - í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ë³´ì • (scale_pos_weight)
    - ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ regularization
    """
    X = df[features]
    y = df['target']

    # ì‹œê³„ì—´ì´ë¯€ë¡œ shuffle=False
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False
    )

    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • (ë§¤ìˆ˜ ì‹ í˜¸ëŠ” ì „ì²´ì˜ ì¼ë¶€)
    pos_count = y_train.sum()
    neg_count = len(y_train) - pos_count
    scale_weight = neg_count / pos_count if pos_count > 0 else 1.0

    model = XGBClassifier(
        n_jobs=-1,
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        min_child_weight=5,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,          # L1 ì •ê·œí™”
        reg_lambda=1.0,         # L2 ì •ê·œí™”
        scale_pos_weight=scale_weight,
        eval_metric='logloss',
        random_state=42,
    )

    print(f"ğŸ§  XGBoost í•™ìŠµ ì‹œì‘ (í”¼ì²˜ {len(features)}ê°œ, ë°ì´í„° {len(X_train)}í–‰)")
    print(f"   í´ë˜ìŠ¤ ë¹„ìœ¨ - ë§¤ìˆ˜ì‹ í˜¸: {pos_count}ê°œ ({pos_count/len(y_train)*100:.1f}%) / ëŒ€ê¸°: {neg_count}ê°œ")

    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False,
    )

    # í‰ê°€
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)

    print(f"ğŸ¯ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {acc:.2%}")
    print(classification_report(
        y_test, preds, target_names=['ëŒ€ê¸°', 'ë§¤ìˆ˜ì‹ í˜¸'], zero_division=0
    ))

    # í”¼ì²˜ ì¤‘ìš”ë„ ìƒìœ„ 10ê°œ
    importances = model.feature_importances_
    feat_imp = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)
    print("ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ TOP 10:")
    for name, imp in feat_imp[:10]:
        bar = "â–ˆ" * int(imp * 50)
        print(f"   {name:15s} {imp:.3f} {bar}")

    return model


def predict_signal(model, row_data, features, threshold=0.60):
    """
    ë‹¨ì¼ ìº”ë“¤ ë°ì´í„°ì— ëŒ€í•´ ë§¤ìˆ˜ ì‹ í˜¸ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

    Returns:
        (signal, probability)
        signal: 'BUY' | 'HOLD'
        probability: ìƒìŠ¹ í™•ë¥  (0.0 ~ 1.0)
    """
    input_df = pd.DataFrame([row_data[features].values], columns=features)
    prob = model.predict_proba(input_df)[0][1]

    if prob >= threshold:
        return 'BUY', prob
    return 'HOLD', prob


def save_model(model, filename="trading_brain.json"):
    model.save_model(filename)
    print(f"ğŸ’¾ ëª¨ë¸ì„ '{filename}'ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


def load_model(filename="trading_brain.json"):
    if not os.path.exists(filename):
        print(f"âš ï¸ ì €ì¥ëœ ëª¨ë¸ '{filename}'ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    loaded = XGBClassifier()
    loaded.load_model(filename)
    print(f"ğŸ“‚ ì €ì¥ëœ ëª¨ë¸ '{filename}'ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    return loaded
